---
title: Sigmoid dÆ°á»›i gÃ³c nhÃ¬n xÃ¡c suáº¥t
layout: post
post-image: "https://images.viblo.asia/100x100/38326b12-21d0-4a55-b299-70c29eca1c2c.png"
description: Giáº£i thÃ­ch cÃ´ng thá»©c toÃ¡n há»c cá»§a Sigmoid trong Machine Learning
tags:
- sample
- post
- test
---

Trong cÃ¡c bÃ i toÃ¡n phÃ¢n lá»›p (classification) sá»­ dá»¥ng Deep Learning, pháº§n khÃ´ng thá»ƒ thiáº¿u cho Ä‘áº§u ra chÃ­nh lÃ  hÃ m sigmoid (cho 2 classes) hay softmax (biáº¿n thá»ƒ cá»§a sigmoid cho nhiá»u classes). Má»¥c Ä‘Ã­ch cá»§a hÃ m sigmoid lÃ  cho ra output lÃ  má»™t vector, trong Ä‘Ã³ tá»•ng cÃ¡c pháº§n tá»­ báº±ng 1, biá»ƒu thá»‹ xÃ¡c suáº¥t kháº£ nÄƒng mÃ  sample Ä‘ang xÃ©t rÆ¡i vÃ o tá»«ng class. Váº­y táº¡i sao sigmoid láº¡i cÃ³ thá»ƒ biá»ƒu diá»…n xÃ¡c suáº¥t, chÃºng ta cÃ¹ng lÃ m rÃµ trong bÃ i viáº¿t nÃ y ğŸ˜„.

## 1. XÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n vÃ  Bayes 
XÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n (Conditional probability) lÃ  xÃ¡c suáº¥t cá»§a má»™t sá»± kiá»‡n A nÃ o Ä‘Ã³, biáº¿t ráº±ng má»™t sá»± kiá»‡n B Ä‘Ã£ xáº£y ra vÃ  Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  $P(A|B)$. Trong Deep Learning, cá»¥ thá»ƒ lÃ  bÃ i toÃ¡n classification, ta cáº§n pháº£i Ä‘i tÃ¬m xÃ¡c suáº¥t rÆ¡i vÃ o má»™t class nÃ o Ä‘Ã³ vá»›i Ä‘iá»u kiá»‡n Ä‘Ã£ biáº¿t lÃ  vector Ä‘áº·c trÆ°ng cá»§a Ä‘áº§u vÃ o. MÃ´ táº£ rÃµ hÆ¡n thÃ¬ vá»›i bÃ i toÃ¡n cáº§n phÃ¢n loáº¡i n classes $C_1, C_2,\dots, C_n$, mÃ´ hÃ¬nh nháº­n Ä‘áº§u vÃ o (áº£nh, text, Ã¢m thanh), cÃ¡c layers trong network sáº½ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng cá»§a Ä‘áº§u vÃ o vÃ  cho ra má»™t vector embedding $x$. Vector embedding nÃ y sáº½ Ä‘iá»u kiá»‡n Ä‘á»ƒ tÃ­nh xÃ¡c suáº¥t rÆ¡i vÃ o class nÃ o. Vá»›i $c_i$ lÃ  class thá»© $i$, ta cÃ³ thá»ƒ kÃ½ hiá»‡u mÃ´ hÃ¬nh xÃ¡c suáº¥t nÃ y lÃ  $P(C_i|x)$.

Quay láº¡i vá» pháº§n lÃ½ thuyáº¿t xÃ¡c suáº¥t, ta cáº§n nháº¯c láº¡i cÃ´ng thá»©c Bayes, Ä‘Ã¢y lÃ  má»™t lÃ½ thuyáº¿t quan trá»ng trong machine learning. TrÆ°á»›c háº¿t ta cáº§n Ä‘á»‹nh nghÄ©a má»™t nhÃ³m Ä‘áº§y Ä‘á»§ nhÆ° sau: NhÃ³m cÃ¡c sá»± kiá»‡n $A_1, A_2,\dots,A_n$ trong Ä‘Ã³ $n\geq2$ táº¡o thÃ nh má»™t nhÃ³m Ä‘áº§y Ä‘á»§ náº¿u
* $A_i$ vÃ  $A_j$ xung kháº¯c tá»«ng Ä‘Ã´i $\forall i\neq j$ hay kÃ½ hiá»‡u lÃ  $A_iA_j=V$ vá»›i $V$ lÃ  sá»± kiá»‡n báº¥t kháº£
* $A_1 +  A_2 + \dots + A_n = U$ (Vá»›i $U$ lÃ  sá»± kiá»‡n táº¥t yáº¿u)

VÃ­ dá»¥ vá» má»™t nhÃ³m Ä‘áº§y Ä‘á»§ lÃ  gieo má»™t con xÃºc sáº¯c, thÃ¬ 6 sá»± kiá»‡n á»©ng vá»›i má»—i sá»± xuáº¥t hiá»‡n cá»§a 1 máº·t trÃªn tá»•ng sá»‘ 6 máº·t táº¡o thÃ nh má»™t nhÃ³m Ä‘áº§y Ä‘á»§. VÃ  dá»… tháº¥y tá»•ng xÃ¡c suáº¥t má»—i sá»± kiá»‡n xáº£y ra Ä‘á»™c láº­p trong má»™t nhÃ³m Ä‘áº§y Ä‘á»§ báº±ng 1. Äá»‘i chiáº¿u vá»›i bÃ i toÃ¡n classification, vá»›i $C_i$ lÃ  sá»± kiá»‡n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘áº§u vÃ o lÃ  class $i$, ta cÃ³ $C_1, C_2,\dots, C_n$ táº¡o thÃ nh má»™t nhÃ³m Ä‘áº§y Ä‘á»§. Vá»›i nhÃ³m Ä‘áº§y Ä‘á»§ vÃ  H lÃ  má»™t sá»± kiá»‡n nÃ o Ä‘Ã³, ta cÃ³ cÃ´ng thá»©c xÃ¡c suáº¥t Ä‘áº§y Ä‘á»§ nhÆ° sau:
$$P(H) = \sum_{i=i}^nP(A_i)P(H|A_i)$$

Tá»« cÃ´ng thá»©c trÃªn vÃ  cÃ´ng thá»©c nhÃ¢n xÃ¡c suáº¥t ta cÃ³ cÃ´ng thá»©c Bayes:
$$P(A_i|H) = \frac{P(A_i)P(H|A_i)}{\sum_{i=1}^n P(A_i)P(H|A_i)}$$

## 2. Æ¯á»›c lÆ°á»£ng há»£p lÃ½ cá»±c Ä‘áº¡i
BÃ i toÃ¡n Æ°á»›c lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ phÃ¡t biá»ƒu nhÆ° sau: Cho biáº¿n ngáº«u nhiÃªn $X$ cÃ³ luáº­t phÃ¢n phá»‘i xÃ¡c suáº¥t Ä‘Ã£ biáº¿t nhÆ°ng chÆ°a biáº¿t tham sá»‘ $\theta$ nÃ o Ä‘Ã³, ta pháº£i xÃ¡c Ä‘á»‹nh giÃ¡ trá»‹ cá»§a $\theta$ dá»±a trÃªn cÃ¡c thÃ´ng tin thu Ä‘Æ°á»£c tá»« máº«u quan sÃ¡t $x_1, x_2,\dots, x_n$ cá»§a $X$. QuÃ¡ trÃ¬nh Ä‘i xÃ¡c Ä‘á»‹nh má»™t tham sá»‘ $\theta$ chÆ°a biáº¿t Ä‘Æ°á»£c gá»i lÃ  quÃ¡ trÃ¬nh Æ°á»›c lÆ°á»£ng tham sá»‘ .
Æ¯á»›c lÆ°á»£ng há»£p lÃ½ cá»±c Ä‘áº¡i (Tiáº¿ng Anh: Maximum likelihood estimation, viáº¿t táº¯t: MLE) lÃ  má»™t phÆ°Æ¡ng phÃ¡p Æ°á»›c tÃ­nh cÃ¡c tham sá»‘ cá»§a phÃ¢n phá»‘i xÃ¡c suáº¥t giáº£ Ä‘á»‹nh, dá»±a trÃªn má»™t sá»‘ dá»¯ liá»‡u quan sÃ¡t. Äiá»u nÃ y Ä‘áº¡t Ä‘Æ°á»£c báº±ng cÃ¡ch tá»‘i Ä‘a hÃ³a hÃ m há»£p lÃ½ (likelihood) sao cho, theo mÃ´ hÃ¬nh thá»‘ng kÃª giáº£ Ä‘á»‹nh, dá»¯ liá»‡u quan sÃ¡t lÃ  cÃ³ xÃ¡c suáº¥t xáº£y ra lá»›n nháº¥t. NguyÃªn lÃ½ há»£p lÃ½ nháº¥t lÃ  tÃ¬m giÃ¡ trá»‹ cá»§a $\theta$ lÃ  hÃ m cá»§a quan sÃ¡t $(x_1,\dots,x_n)$ sao cho báº£o Ä‘áº£m xÃ¡c suáº¥t thu Ä‘Æ°á»£c quan sÃ¡t Ä‘Ã³ lÃ  lá»›n nháº¥t. Giáº£ sá»­ biáº¿n gá»‘c $X$ cÃ³ hÃ m máº­t Ä‘á»™ $f(x, \theta)$, khi Ä‘Ã³ hÃ m há»£p lÃ½ (likelihood) Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau:
$$L(x, \theta) = \prod_{i=1}^nf(x, \theta)$$

Náº¿u hÃ m likelihood Ä‘áº£m báº£o Ä‘iá»u kiá»‡n kháº£ vi 2 láº§n , ta cÃ³ Ä‘iá»u kiá»‡n cáº§n Ä‘á»ƒ cÃ³ cá»±c trá»‹:
$$\frac{\partial L(x, \theta)}{\partial\theta}=0$$
tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i
$$\frac{\partial\ln L(x, \theta)}{\partial\theta}=0$$
PhÆ°Æ¡ng trÃ¬nh trÃªn cÅ©ng Ä‘Æ°á»£c gá»i lÃ  phÆ°Æ¡ng trÃ¬nh há»£p lÃ½ nháº¥t. 

*Note: Thá»±c táº¿ viá»‡c tÃ¬m Æ°á»›c lÆ°á»£ng há»£p lÃ½ nháº¥t ráº¥t khÃ³ khÄƒn do hÃ m likelihood khÃ´ng pháº£i lÃºc nÃ o cÅ©ng lá»“i vÃ  thÆ°á»ng phi tuyáº¿n. Trong pháº¡m vi bÃ i viáº¿t mÃ¬nh sáº½ khÃ´ng Ä‘á» cáº­p quÃ¡ sÃ¢u pháº§n nÃ y, cÃ¡c báº¡n cÃ³ thá»ƒ tÃ¬m hiá»ƒu thÃªm*

## 3. MÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh dÆ°á»›i gÃ³c nhÃ¬n xÃ¡c suáº¥t
LÃ½ do Ä‘á» cáº­p Ä‘áº¿n há»“i quy tuyáº¿n tÃ­nh á»Ÿ Ä‘Ã¢y lÃ  Ä‘á»ƒ xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ xem nÃ³ nhÆ° má»™t mÃ´ hÃ¬nh xÃ¡c suáº¥t cá»§a dá»¯ liá»‡u vÃ  liá»‡u chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c Ã½ tÆ°á»Ÿng tÆ°Æ¡ng tá»± vÃ o viá»‡c phÃ¢n loáº¡i hay khÃ´ng. Giáº£ sá»­ ta cÃ³ mÃ´ hÃ¬nh tuyáº¿n tÃ­nh sau
$$y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}$$
Trong Ä‘Ã³ $\epsilon$ lÃ  nhiá»…u vÃ  Ä‘á»™c láº­p vá»›i $x$. Theo Äá»‹nh lÃ½ giá»›i háº¡n trung tÃ¢m (Äá»‹nh lÃ½ nÃ y lÃ  káº¿t quáº£ vá» sá»± há»™i tá»¥ yáº¿u cá»§a má»™t dÃ£y cÃ¡c biáº¿n ngáº«u nhiÃªn, theo Ä‘Ã³ tá»•ng cá»§a cÃ¡c biáº¿n ngáº«u nhiÃªn Ä‘á»™c láº­p vÃ  phÃ¢n phá»‘i Ä‘á»“ng nháº¥t theo cÃ¹ng má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t, sáº½ há»™i tá»¥ vá» má»™t biáº¿n ngáº«u nhiÃªn nÃ o Ä‘Ã³) thÃ¬ $\epsilon$ tuÃ¢n theo phÃ¢n phá»‘i chuáº©n. á» Ä‘Ã¢y $\epsilon$ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ biá»ƒu thá»‹ sá»± sai lá»‡ch giá»¯a target vÃ  káº¿t quáº£ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh, vÃ  nÃ³ cÃ³ hÃ m phÃ¢n phá»‘i
$$P(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$$
$$P(y^{(i)}|x^{(i)}; \theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
Ta gá»i Ä‘Ã¢y lÃ  xÃ¡c suáº¥t xáº£y ra $y^{(i)}$ vá»›i Ä‘iá»u kiá»‡n $x^{(i)}$ Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi $\theta$. á» Ä‘Ã¢y bá»™ trá»ng sá»‘ $\theta$ lÃ  biáº¿n ngáº«u nhiÃªn vÃ  Ä‘Æ°á»£c tá»‘i Æ°u trong quÃ¡ trÃ¬nh mÃ´ hÃ¬nh há»c. Má»™t cÃ¡ch tá»± nhiÃªn, má»¥c Ä‘Ã­ch cá»§a quÃ¡ trÃ¬nh há»c lÃ  tÃ¬m bá»™ tham sá»‘ $\theta$ sao cho vá»›i Ä‘áº·c trÆ°ng quan sÃ¡t $x^{(i)}$, kháº£ nÄƒng nÃ³ rÆ¡i vÃ o target $y^{(i)}$ lÃ  lá»›n nháº¥t (Äáº¿n Ä‘Ã¢y chÃºng ta Ä‘Ã£ tháº¥y sá»± liÃªn quan tá»›i lÃ½ thuyáº¿t MLE á»Ÿ bÃªn trÃªn rá»“i ha ğŸ˜…). Tiáº¿p theo ta Ä‘á»‹nh nghÄ©a hÃ m likelihood cá»§a $\theta$  
$$L(\theta)=L(\theta; X, \hat{y}) = P(\hat{y}|X; \theta)$$
Theo phÆ°Æ¡ng trÃ¬nh há»£p lÃ½ nháº¥t á»Ÿ pháº§n trÃªn, ta cÃ³
$$L(\theta)=\prod_{i=1}^nP(y^{(i)}|x^{(i)}; \theta)$$
$$L(\theta)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
Äá»ƒ dá»… dÃ ng hÆ¡n cho viá»‡c tá»‘i Æ°u, ta láº¥y logarithm hai váº¿ (viá»‡c nÃ y okela vÃ¬ hÃ m log lÃ  Ä‘Æ¡n Ä‘iá»‡u) Ä‘á»ƒ biáº¿n viá»‡c tá»‘i Æ°u hÃ m mÅ© thÃ nh tá»‘i Æ°u hÃ m Ä‘a thá»©c. Do Ä‘Ã³ quÃ¡ trÃ¬nh tá»‘i Æ°u MLE sá»­ dá»¥ng hÃ m cÃ³ tÃªn gá»i lÃ  log-likelihood.
$$l(\theta) = \log L(\theta)$$
$$= \log\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
$$= \prod_{i=1}^n\log\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
$$= n\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma^2}\cdot\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\theta^Tx^{(i)})^2$$
Cá»±c Ä‘áº¡i hÃ³a biá»ƒu thá»©c trÃªn tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i cá»±c tiá»ƒu hÃ³a biá»ƒu thá»©c sau (Ä‘á»ƒ Ã½ Ä‘Ã¢y chÃ­nh lÃ  cÃ´ng thá»©c least-square)
$$\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\theta^Tx^{(i)})^2$$
## 4. XÃ¢y dá»±ng cÃ´ng thá»©c cho sigmoid
á» pháº§n 3. Ä‘Ã£ map má»™t bá»™ dá»± Ä‘oÃ¡n tuyáº¿n tÃ­nh vá»›i nhiá»…u Gauss tá»›i biáº¿n má»¥c tiÃªu. Äá»‘i vá»›i bÃ i toÃ¡n binary classification, sáº½ tháº­t tuyá»‡t náº¿u chÃºng ta cÃ³ thá»ƒ lÃ m Ä‘iá»u gÃ¬ Ä‘Ã³ tÆ°Æ¡ng tá»±, tá»©c lÃ  map má»™t bá»™ dá»± Ä‘oÃ¡n tuyáº¿n tÃ­nh vá»›i má»™t thá»© gÃ¬ Ä‘Ã³ tá»›i xÃ¡c suáº¥t thuá»™c má»™t trong hai lá»›p vÃ  sá»­ dá»¥ng MLE Ä‘á»ƒ giáº£i thÃ­ch cho thiáº¿t káº¿ mÃ´ hÃ¬nh báº±ng viá»‡c nÃ³ tá»‘i Ä‘a hÃ³a xÃ¡c suáº¥t cho viá»‡c rÆ¡i vÃ o 1 class nÃ o Ä‘Ã³ cá»§a 1 Ä‘áº·c trÆ°ng quan sÃ¡t.

XÃ©t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh $y=\theta^Tx^{(i)}+\epsilon^{(i)}$, má»™t táº­p cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thuá»™c 2 classes $C_1$ vÃ  $C_2$ tuÃ¢n theo hai phÃ¢n phá»‘i chuáº©n cÃ³ ká»³ vá»ng $\mu_1$ vÃ  $\mu_2$, Ä‘á»™ lá»‡ch chuáº©n cÃ¹ng báº±ng 1 (á» Ä‘Ã¢y mÃ¬nh chá»n 1 cho thuáº­n tiá»‡n viá»‡c biáº¿n Ä‘á»•i biá»ƒu thá»©c, vÃ¬ thá»±c ra chá»n $\sigma$ báº¥t ká»³ thÃ¬ káº¿t quáº£ cuá»‘i cÃ¹ng váº«n ra má»™t Ä‘a thá»©c nhÆ°ng nhÃ¬n nÃ³ sáº½ rá»‘i máº¯t khÃ´ng cáº§n thiáº¿t). Ta cáº§n phÃ¢n loáº¡i xem vá»›i vá»›i vector Ä‘áº·c trÆ°ng $x$, nÃ³ sáº½ rÆ¡i vÃ o class nÃ o trong 2 classes $C_1$ vÃ  $C_2$. Theo cÃ´ng thá»©c Bayes, ta cÃ³
$$P(C_1|x) = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}$$
Chia cáº£ tá»­ vÃ  máº«u cho $P(x|C_1)P(C_1)$ ta cÃ³: 
$$P(C_1|x) = \frac{1}{1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}$$
MÃŒnh sáº½ Ä‘á»ƒ láº¡i nÃ³ má»™t chÃºt, quay láº¡i vá» cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u tuÃ¢n theo hai phÃ¢n phá»‘i chuáº©n vá»«a Ä‘á» cáº­p bÃªn trÃªn, káº¿t há»£p sá»­ dá»¥ng Æ°á»›c lÆ°á»£ng há»£p lÃ½ cá»±c Ä‘áº¡i, ta cÃ³
$$P(x|C_1) = N(\mu_1|1) \sim \exp(-\frac{(x-\mu_1)^2}{2})$$
$$P(x|C_2) = N(\mu_2|1) \sim \exp(-\frac{(x-\mu_2)^2}{2})$$
Chia $P(X|C_2)$ cho $P(X|C_1)$ ta Ä‘Æ°á»£c:
$$ \frac{P(X|C_2)}{P(X|C_1)}=\exp(\frac{(x-\mu_1)^2-(x-\mu_2)^2}{2})$$
$$=\exp((\mu_2-\mu_1)x-\frac{1}{2}(\mu_2^2-\mu_1^2))$$
Äáº·t $(\mu_2-\mu_1)x-\frac{1}{2}(\mu_2^2-\mu_1^2)=-\alpha(x)$, ta thu Ä‘Æ°á»£c
$$P(C_1|x) = \frac{1}{1+\exp(-\alpha(x))} = \sigma\circ\alpha(x) $$
Trong Ä‘Ã³ $\sigma: R\to (0,1)$ Ä‘Æ°á»£c gá»i lÃ  hÃ m sigmoid vÃ  $\alpha: R^d\to R$ Ä‘Æ°á»£c cho bá»Ÿi cÃ´ng thá»©c
$$\alpha(x)=\log\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}=\log\frac{P(C_1, x)}{P(C_2, x)}$$
Váº­y lÃ  cÃ´ng thá»©c hÃ m sigmoid Ä‘Ã£ xuáº¥t hiá»‡n =))). Trong trÆ°á»ng há»£p tá»•ng quÃ¡t cho nhiá»u class $C_1,\dots, C_n$, ta cÃ³
$$P(C_k|x)=\frac{P(x|C_k)P(C_k)}{\sum_{j=1}^{n}P(x|C_j)P(C_j)}=\frac{\exp(\alpha_k(x))}{\sum_{j=1}^{n}\exp(\alpha_j(x))}$$
ÄÃ¢y chÃ­nh lÃ  cÃ´ng thá»©c cá»§a hÃ m softmax!
## TÃ i liá»‡u tham kháº£o
1. Tá»‘ng ÄÃ¬nh Quá»³. *GiÃ¡o trÃ¬nh xÃ¡c suáº¥t thá»‘ng kÃª*. NXB BÃ¡ch Khoa HÃ  Ná»™i
2. C. M. Bishop. *Pattern recognition and machine learning*. Springer, 2006.
